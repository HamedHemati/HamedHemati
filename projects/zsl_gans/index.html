<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zero-Shot Classification and Retrieval Using GANs | Hamed Hemati </title> <meta name="author" content="Hamed Hemati"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hamedhemati.github.io/projects/zsl_gans/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hamed</span> Hemati </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Zero-Shot Classification and Retrieval Using GANs</h1> <p class="post-meta"> Created in December 28, 2018 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2018   ·   <i class="fa-solid fa-tag fa-sm"></i> thesis </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this blog post, I’ll share interesting highlights in my journey of exploring and implementing Zero-Shot Learning (ZSL) using Generative Adversarial Networks (GANs). I worked on this project back in 2018 for master’s thesis in the Computer Vision Lab at the University of Bern, Switzerland.</p> <h2 id="motivation">Motivation</h2> <hr> <p>Humans have an incredible ability to learn new concepts without needing explicit examples. Zero-Shot Learning aims to replicate this ability by solving tasks for categories that were not seen during training. The key idea is leveraging auxiliary information, such as textual descriptions or attribute vectors, to imagine unseen classes.</p> <p>For example, given the description: “<em>A lion is a muscular, deep-chested cat with a short, rounded head,</em>” a ZSL model should be able to identify lions without seeing a single image of a lion.</p> <h2 id="overview-of-zero-shot-learning">Overview of Zero-Shot Learning</h2> <hr> <p>In ZSL, the model transfers knowledge from seen classes to unseen classes using a shared representation. In this project, I focused three scenarios in ZSL:</p> <ol> <li> <strong>Standard ZSL</strong>: Classification of only unseen classes at test time.</li> <li> <strong>Generalized ZSL</strong>: Classification of both seen and unseen classes.</li> <li> <strong>Zero-Shot Retrieval</strong>: Retrieving images of unseen classes.</li> </ol> <h2 id="generative-adversarial-networks-for-zsl">Generative Adversarial Networks for ZSL</h2> <hr> <p>Generative approaches emerged as an alternative solution to ZSL. GANs, which are popular generative models, can be used to synthesize visual features for unseen categories based on auxiliary information. The key idea is simple yet powerful: If we can generate features that resemble unseen class distributions, we can train a classifier on these features. This requires a feature extractor that can be exploited for extracting features from images both at training and test times.</p> <h2 id="methodology">Methodology</h2> <hr> <h3 id="gan-architecture">GAN Architecture</h3> <p>I experimented with multiple GAN variations and ultimately designed two architectures:</p> <ol> <li> <strong>FiLM-WGAN</strong>: Incorporates Feature-wise Linear Modulation (FiLM) to condition the generator and discriminator on class embeddings.</li> <li> <strong>ZS-WGAN</strong>: A simpler, robust architecture inspired by the f-CLSWGAN model.</li> </ol> <h4 id="film-wgan">FiLM-WGAN</h4> <p>Feature-wise Linear Modulation applies affine transformations conditioned on the class embedding:</p> \[FiLM(x, z) = \gamma(z) \odot x + \beta(z)\] <p>Where:</p> <ul> <li>\(x\): Input feature vector</li> <li>\(z\): Class embedding</li> <li>\(\gamma(z), \beta(z)\): Learnable scale and shift parameters</li> </ul> <p>This module can be used to incorporate the conditional information more effectively, compared to naive concatenation of the conditoner with the input.</p> <h2 id="why-wasserstein-gans">Why Wasserstein GANs?</h2> <hr> <p>Traditional GANs are often unstable during training due to issues with mode collapse and convergence. Wasserstein GANs (WGANs) address these issues by introducing Wasserstein Distance, also known as Earth Mover’s Distance (EMD), as the metric for comparing distributions.</p> <h3 id="wasserstein-distance">Wasserstein Distance</h3> <p>The Wasserstein Distance between two probability distributions \(P_r\) (real) and \(P_g\) (generated) is defined as:</p> \[W(P_r, P_g) = \inf_{\gamma \sim \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [\| x - y \|]\] <p>Where:</p> <ul> <li>\(\Pi(P_r, P_g)\): Set of all joint distributions \(\gamma\) with marginals \(P_r\) and \(P_g\).</li> <li>\(\| x - y \|\): Cost of transporting mass from \(x\) to \(y\).</li> </ul> <p>Instead of directly calculating the infimum, WGANs use the Kantorovich-Rubinstein duality:</p> \[W(P_r, P_g) = \frac{1}{K} \sup_{\|f\|_L \leq K} \mathbb{E}_{x \sim P_r}[f(x)] - \mathbb{E}_{x \sim P_g}[f(x)]\] <p>Here, \(f\) is a K-Lipschitz continuous function learned by the discriminator, ensuring a smoother regularization.</p> <h3 id="key-advantages-of-wgans">Key Advantages of WGANs</h3> <ol> <li> <strong>Stable Training</strong>: Replacing the discriminator with a critic that estimates Wasserstein Distance improves gradient behavior.</li> <li> <strong>No Mode Collapse</strong>: WGANs provide meaningful gradients even when the generated and real distributions are far apart.</li> <li> <strong>Improved Convergence</strong>: By enforcing a 1-Lipschitz constraint via gradient penalty, training becomes smoother and more robust.</li> </ol> <h3 id="gradient-penalty">Gradient Penalty</h3> <p>To enforce the Lipschitz constraint, WGAN-GP adds gradient penalty term below:</p> \[GP = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}} [(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]\] <p>Where \(\hat{x}\) is a random sample interpolated between real and fake data.</p> <h3 id="training-pipeline">Training Pipeline</h3> <hr> <p>The training involves:</p> <ol> <li> <strong>Feature Extraction</strong>: Using a pre-trained (ResNet-101) to extract visual features.</li> <li> <strong>Feature Generation</strong>: Training a GAN to generate features for unseen classes.</li> <li> <strong>Classifier Training</strong>: A classifier is trained on the synthesized features.</li> </ol> <p>For training the GAN, a combined loss function is used:</p> \[L = W(P_r, P_g) + \alpha \cdot L_{cls}\] <p>Where \(L_{cls}\) is the classification loss that classifies generated features.</p> <h2 id="experiments-and-results">Experiments and Results</h2> <hr> <p>I evaluated the models on three datasets:</p> <ul> <li> <strong>AWA2 (Animals with Attributes 2)</strong>: 50 animal classes, each described by attribute vectors.</li> <li> <strong>CUB (Caltech-UCSD Birds)</strong>: 200 bird species with fine-grained attributes.</li> </ul> <h3 id="generalized-zero-shot-classification-results">Generalized Zero-Shot Classification Results</h3> <table> <thead> <tr> <th><strong>Method</strong></th> <th><strong>AWA1 (U)</strong></th> <th><strong>AWA1 (S)</strong></th> <th><strong>AWA1 (H)</strong></th> <th><strong>AWA2 (U)</strong></th> <th><strong>AWA2 (S)</strong></th> <th><strong>AWA2 (H)</strong></th> <th><strong>CUB (U)</strong></th> <th><strong>CUB (S)</strong></th> <th><strong>CUB (H)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Non-generative Models</strong></td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>CONSE</td> <td>0.4</td> <td>88.6</td> <td>0.8</td> <td>0.5</td> <td>90.6</td> <td>1.0</td> <td>1.6</td> <td>72.2</td> <td>3.1</td> </tr> <tr> <td>LATEM</td> <td>7.3</td> <td>71.7</td> <td>13.3</td> <td>11.5</td> <td>77.3</td> <td>20.0</td> <td>15.2</td> <td>57.3</td> <td>24.0</td> </tr> <tr> <td>DAP</td> <td>0.0</td> <td>88.7</td> <td>0.0</td> <td>0.0</td> <td>84.7</td> <td>0.0</td> <td>1.7</td> <td>67.9</td> <td>3.3</td> </tr> <tr> <td>IAP</td> <td>2.1</td> <td>78.2</td> <td>4.1</td> <td>0.9</td> <td>87.6</td> <td>1.8</td> <td>0.2</td> <td>72.8</td> <td>0.4</td> </tr> <tr> <td>ALE</td> <td>16.8</td> <td>76.1</td> <td>27.5</td> <td>14.0</td> <td>81.8</td> <td>23.9</td> <td>23.7</td> <td>62.8</td> <td>34.4</td> </tr> <tr> <td>SYNC</td> <td>8.9</td> <td>87.3</td> <td>16.2</td> <td>10.0</td> <td>90.5</td> <td>18.0</td> <td>11.5</td> <td>70.9</td> <td>19.8</td> </tr> <tr> <td>ESZSL</td> <td>6.6</td> <td>75.6</td> <td>12.1</td> <td>5.9</td> <td>77.8</td> <td>11.0</td> <td>12.6</td> <td>63.8</td> <td>21.0</td> </tr> <tr> <td>DEVISE</td> <td>13.4</td> <td>68.7</td> <td>22.4</td> <td>17.1</td> <td>74.7</td> <td>27.8</td> <td>23.8</td> <td>53.0</td> <td>32.8</td> </tr> <tr> <td>SJE</td> <td>11.3</td> <td>74.6</td> <td>19.6</td> <td>8.0</td> <td>73.9</td> <td>14.4</td> <td>23.5</td> <td>59.2</td> <td>33.6</td> </tr> <tr> <td><strong>Generative Models</strong></td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>f-CLSWGAN</td> <td>57.9</td> <td>61.4</td> <td>59.0</td> <td>53.8</td> <td>68.2</td> <td>60.2*</td> <td>43.7</td> <td>57.7</td> <td>49.7</td> </tr> <tr> <td>SE-GZSL</td> <td>56.3</td> <td>67.8</td> <td>61.5</td> <td>58.3</td> <td>68.1</td> <td><strong>62.8</strong></td> <td>41.5</td> <td>53.5</td> <td>46.7</td> </tr> <tr> <td>CVAE-ZSL</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>51.2</td> <td>-</td> <td>-</td> <td>34.5</td> </tr> <tr> <td>Vanilla Conditional GAN</td> <td>50.3</td> <td>64.7</td> <td>56.6</td> <td>48.8</td> <td>65.4</td> <td>55.9</td> <td>34.3</td> <td>42.1</td> <td>37.8</td> </tr> <tr> <td><strong>Our Models</strong></td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>ZS-WGAN</td> <td>57.5</td> <td>66.3</td> <td><strong>61.6</strong></td> <td>54.2</td> <td>71.5</td> <td>61.7</td> <td>39.9</td> <td>50.9</td> <td>44.7</td> </tr> <tr> <td>FiLM-WGAN</td> <td>56.8</td> <td>66.0</td> <td>61.0</td> <td>57.0</td> <td>67.9</td> <td>62.0</td> <td>47.2</td> <td>55.5</td> <td><strong>51.0</strong></td> </tr> </tbody> </table> <p><strong>Notes</strong>:</p> <ul> <li>U: Unseen classes</li> <li>S: Seen classes</li> <li>H: Harmonic mean of U and S</li> </ul> <p>From the table, it’s clear that generative models outperform non-generative models in all settings of generalized zero-shot classification. Non-generative methods like CONSE, DAP, and IAP struggle significantly with unseen classes (U), resulting in very low harmonic means (H). Generative models, on the other hand, show much better performance due to their ability to synthesize realistic features for unseen classes. Among the generative models:</p> <ul> <li> <strong>ZS-WGAN and FiLM-WGAN</strong> (our models) achieve competitive results, especially in the harmonic mean. For example, ZS-WGAN achieves an <strong>H of 61.6%</strong> on AWA1 and <strong>61.7%</strong> on AWA2, which is higher than many baseline methods.</li> <li>FiLM-WGAN, leveraging Feature-wise Linear Modulation, performs exceptionally well on the CUB dataset, achieving an <strong>H of 51.0%</strong>, surpassing all other methods.</li> </ul> <p>Interestingly, our models also maintain a good balance between unseen (U) and seen (S) class performance, which is crucial in generalized zero-shot settings.</p> <p><strong>Takeaway</strong>: Generative approaches, particularly our models (ZS-WGAN and FiLM-WGAN), demonstrate their effectiveness in bridging the gap between seen and unseen classes, which enables robust classification in challenging zero-shot scenarios.</p> <h2 id="summary">Summary</h2> <hr> <p>In this project I attempted to demonstrate the potentials of generative modesl (GANs) in Zero-Shot Learning. By synthesizing visual features, we can overcome some aspects of data limitation and create robust classifiers for unseen classes. There are limitations to this approach though. The biggest limitation is the quality of the feature extrator, in particular when generating features for samples from unseen classes at test time. Additonally, convergence issues in GANs can add up to the difficulty of training such models. That said, ZSL through “pseudo-imagination”, is an exciting approach to the problem, and is definitely worth further investigation. Feel free to check out the full code <a href="https://github.com/HamedHemati/Master-Thesis" rel="external nofollow noopener" target="_blank">on my GitHub</a>.</p> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hamed Hemati. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-stateless-models-in-pytorch",title:"Stateless Models in PyTorch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/tabs/"}},{id:"news-i-will-be-challenge-chair-at-the-clvision-2023-workshop",title:"I will be challenge chair at the CLVision 2023 workshop",description:"",section:"News"},{id:"news-two-papers-accepted-at-collas-2023",title:"Two papers accepted at CoLLAs-2023",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2023_collas/"}},{id:"news-avalanche-has-been-accepted-by-jmlr-machine-learning-open-source-software",title:"Avalanche has been accepted by JMLR - Machine Learning Open Source Software",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2023_jmlr/"}},{id:"news-i-will-be-challenge-chair-at-the-clvision-2024-workshop",title:"I will be challenge chair at the CLVision 2024 workshop",description:"",section:"News"},{id:"news-clvision-challenge-report-is-published-in-elsevier-39-s-neural-networks",title:"CLVision challenge report is published in Elsevier&#39;s Neural Networks",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2024_neuralnets/"}},{id:"projects-text-to-speech-for-podcast-generation",title:"Text-to-Speech for Podcast Generation",description:"",section:"Projects",handler:()=>{window.location.href="/projects/vocally_yours/"}},{id:"projects-zero-shot-classification-and-retrieval-using-gans",title:"Zero-Shot Classification and Retrieval Using GANs",description:"",section:"Projects",handler:()=>{window.location.href="/projects/zsl_gans/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%65%6D%61%74%69.%68%6D%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=IVJi7QgAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HamedHemati","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>