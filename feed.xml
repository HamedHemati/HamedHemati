<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://hamedhemati.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hamedhemati.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-18T13:06:10+00:00</updated><id>https://hamedhemati.github.io/feed.xml</id><title type="html">Hamed Hemati</title><subtitle></subtitle><entry><title type="html">Stateless Models in PyTorch</title><link href="https://hamedhemati.github.io/blog/2023/stateless_models/" rel="alternate" type="text/html" title="Stateless Models in PyTorch"/><published>2023-08-12T00:00:00+00:00</published><updated>2023-08-12T00:00:00+00:00</updated><id>https://hamedhemati.github.io/blog/2023/stateless_models</id><content type="html" xml:base="https://hamedhemati.github.io/blog/2023/stateless_models/"><![CDATA[<p>In the realm of deep learning libraries, the topic of <em>state representation</em> of a model becomes crucial in certain problems. Broadly speaking, there are two widely used state representation methods: <strong>stateful</strong> and <strong>stateless</strong>.</p> <p>In this article, I’ll briefly introduce the concept and demonstrate through examples when to use which method when implementing a solution in PyTorch. Some topics related to this article are:</p> <ul> <li>Meta-learning</li> <li>Hypernetworks</li> <li>Ensemble Modelling</li> </ul> <h2 id="what-are-stateful-and-stateless-models">What are Stateful and Stateless Models?</h2> <hr/> <p>Getting straight to the point, in PyTorch, the two state representations are defined as follows:</p> <p><strong>Stateful Models</strong> are models whose state (aka the model’s weights) is defined and stored inside the model class definition. When you create an instance of the model, it initializes the wieghts internally. <u>By default, PyTorch models are stateful</u>. Below is an example of a stateful MLP with one hidden layer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">StatefulModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">StatefulModel</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1"># Initialize model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">StatefulModel</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize a random input
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Make prediction
</span><span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>In the example, the layers are defined as the module attributes of the class, specifically of type <code class="language-plaintext highlighter-rouge">nn.Linear</code>. The definition of each layer contains the set of operations needed for forward propagation and also maintains the state for that layer. As a results, the entire model posseses its complete state which can be access via <code class="language-plaintext highlighter-rouge">model.state_dict()</code>. In Pytorch, model’s weights are stored as an <code class="language-plaintext highlighter-rouge">Ordered Dictionary</code>.</p> <p><strong>Stateless Models</strong>, on the other hand, do not store weights internally. Instead, the weights are passed as input. The example below shows a stateless definition of a model in PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">StatelessModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">fc1.weight</span><span class="sh">'</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">fc1.bias</span><span class="sh">'</span><span class="p">]))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">fc2.weight</span><span class="sh">'</span><span class="p">],</span> <span class="n">weights</span><span class="p">[</span><span class="sh">'</span><span class="s">fc2.bias</span><span class="sh">'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span>



<span class="c1"># Initialize model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">StatelessModel</span><span class="p">()</span>

<span class="c1"># Initialize weights
</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">fc1.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">fc1.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">fc2.weight</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">fc2.bias</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Initialize random input
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Make prediction
</span><span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</code></pre></div></div> <p>As you can see, in the stateless version, there are no module initializations when creating an instance of the class. Instead, the class only defines the forward function that instructs how an input should be mapped to an output through a set of ternsor operations.</p> <blockquote class="block-warning"> <h5 id="ambiguity">Ambiguity</h5> <p>In the context of Recurrent Neural Networks (RNNs), the terms “stateful” and “stateless” can take on a different meaning, and refer to <u> the way the hidden state of the RNN is initialized</u> . In a stateful RNN, the hidden state is carried over from the previous step. In contrast, a stateless RNN (randomly) re-initializes the hidden state at each step.</p> </blockquote> <h2 id="when-to-use-which">When to Use Which?</h2> <hr/> <blockquote class="block-tip"> <h5 id="tldr">TLDR</h5> <p>“Stateless” models can be used for any type of problem, but they require manual management of the state and often involve more implementation effort. “Stateful” models are easier to implement, but can make life of a researcher or programmer harder in certain use cases.</p> </blockquote> <p>Some libraries like <a href="https://flax.readthedocs.io/en/latest/">Flax</a> (JAX+Flax) assume a stateless model definition by default, while in PyTorch standard model definitions are stateful. In general, if your problem has the “standard” training loop as below:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>REPEAT:
    - get data batch
    - forward an input to the model
    - compute loss and backpropagate
    - update weights
</code></pre></div></div> <p>then you’re probably better of choosing the stateful option. In most instances, stateful versions are more convenient because everything is handled as an all-in-one package.</p> <h3 id="examples">Examples</h3> <p>Despite their user-friendliness, stateful models can be problematic in certain situations. I’ve listed some of these situations, with an illustration for each, below:</p> <blockquote> <p>When weights of the model are generated by another model, or when the weights are perturbed or changed by an external modifier.</p> </blockquote> <p>A popular example is the <a href="https://arxiv.org/abs/1609.09106">hypernetwork</a>. Hypernetworks generate the weights of another network, usually referred to as the “main model”. The main model cannot maintain its state, instead, it receives it as an input provided by the output of the hypernetwork.</p> <div style="display: flex; justify-content: center;"> <div class="mt-3 mt-md-0" style="max-width: 30%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-08-12-stateless_models/hypernet-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-08-12-stateless_models/hypernet-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-08-12-stateless_models/hypernet-1400.webp"/> <img src="/assets/img/posts/2023-08-12-stateless_models/hypernet.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>When the graph of the optimization process has to be maintained.</p> </blockquote> <p>Usually, in the inner loop of meta-optimization methods, we need to keep the computation graph throughout the entire optimization process. To acheive this, the state of the model cannot be stored internally and it needs to be kept externally for reference.</p> <div style="display: flex; justify-content: center;"> <div class="mt-3 mt-md-0" style="max-width: 50%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-08-12-stateless_models/meta_learning-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-08-12-stateless_models/meta_learning-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-08-12-stateless_models/meta_learning-1400.webp"/> <img src="/assets/img/posts/2023-08-12-stateless_models/meta_learning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Parallel inference in ensemble models.</p> </blockquote> <p>Althout its possible to perform inference in ensemble models with stateful version, having multiple copies of the same model for parallel inference can become costly. For example if we need to make prediction with hundreds of models, each with small purterbation applied to a reference model, it can be achieve in more a effcient way with stateless versions.</p> <div style="display: flex; justify-content: center;"> <div class="mt-3 mt-md-0" style="max-width: 40%; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/2023-08-12-stateless_models/ensemble-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/2023-08-12-stateless_models/ensemble-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/2023-08-12-stateless_models/ensemble-1400.webp"/> <img src="/assets/img/posts/2023-08-12-stateless_models/ensemble.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="stateless-models-in-pytorch">Stateless Models in PyTorch</h2> <hr/> <p>Before PyTorch 2.0, converting a stateful model to its stateless equivalent required rewriting the forward function using functionals. An alternative was to use packages such as <a href="https://github.com/pytorch/functorch"><code class="language-plaintext highlighter-rouge">funchtorch</code></a> and <a href="https://github.com/facebookresearch/higher"><code class="language-plaintext highlighter-rouge">higher</code></a> that facilitated this conversion through monkeypatching. Luckily, since version 2.0, there is not need for external modules. A stateful model can be <strong>called</strong> in a stateless manner using <code class="language-plaintext highlighter-rouge">torch.nn.utils.stateless.functional_call</code> as demonstrated in the following example.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.nn.utils.statless</span> <span class="kn">import</span> <span class="n">functionall_call</span>

<span class="c1"># Using the StatefulModel definition in the first code example
# Initialize model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">StatefulModel</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize random input
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Get the state dict of the model
</span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>

<span class="c1"># Stateless call
</span><span class="nf">functional_call</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Make prediction
</span><span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>In the example, we don’t convert the model directly. Instead, we utilize the stateful version and call it in a stateless manner. This minor change in the function call can enable a cleaner implementation of methods, which previously relied on third-party libraries in a more intricate way.</p>]]></content><author><name></name></author><category term="programming"/><category term="machine_learning"/><category term="pytorch"/><category term="machine_learning"/><summary type="html"><![CDATA[In the realm of deep learning libraries, the topic of state representation of a model becomes crucial in certain problems. Broadly speaking, there are two widely used state representation methods: stateful and stateless.]]></summary></entry></feed>